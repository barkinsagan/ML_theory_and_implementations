{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that you are flipping a coin you got 4 heads and 6 tails. What is the probability of getting heads?\n",
    "\n",
    "One reasonable thing to do is calculate \n",
    "\n",
    "#### $ P(heads) = \\frac{n_h}{n_h + n_t}$\n",
    "\n",
    "Based on the data we are estimating the probability of heads to be 4/10 = 0.4.\n",
    "\n",
    "But how do we mathematically justify that?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "The idea of Maximum Likelihood Estimation is to find the parameters that make the data most likely.\n",
    "\n",
    "$P(D;\\theta)$ where $D$ is the data and $\\theta$ is the probability of getting heads.\n",
    "\n",
    "We want to find the $\\theta$ that maximizes the probability of seeing the data\n",
    "\n",
    "$\\theta_{MLE} = argmax_{\\theta}  P(D;\\theta)$\n",
    "\n",
    " $P(D;\\theta) = \\binom{n_h + n_t}{n_h} \\theta^{n_h} (1-\\theta)^{n_t}$ \n",
    "\n",
    "Instead of maximizing the probability of the data, we can maximize the log of the probability of the data.\n",
    "\n",
    "$log P(D;\\theta) = log \\binom{n_h + n_t}{n_h} + n_h log \\theta + n_t log (1-\\theta)$\n",
    "\n",
    "$log P(D;\\theta) = log \\binom{n_h + n_t}{n_h} + n_h log \\theta + n_t log (1-\\theta)$\n",
    "\n",
    "if we take derivative with respect to $\\theta$ and set it to 0, we get\n",
    "\n",
    "$\\frac{d}{d\\theta} log P(D;\\theta) = \\frac{n_h}{\\theta} - \\frac{n_t}{1-\\theta} = 0$\n",
    "\n",
    "$\\theta_{MLE} = \\frac{n_h}{n_h + n_t}$\n",
    "\n",
    "\n",
    "Basically for every parameter $\\theta$ we have a probability distribution $P(D;\\theta)$ and we are looking for the value of $\\theta$ that makes the probability of actually seeing the data the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we might have a small sample size we usually assume that we have seen heads and tails m times each to smooth out the data.\n",
    "\n",
    "In the end we  have \n",
    "\n",
    "$\\theta = (n_h +m_h) / (n_h + n_t + m_h + m_t)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequentist vs Bayesian\n",
    "\n",
    "In frequentist approach we assume theta is a parameter and we try to maximize the probability of $P(D;\\theta)$\n",
    "\n",
    "In Bayesian approach we assume that theta is a random variable and we try to maximize the probability of $\\theta$ given the data $P(\\theta|D)$\n",
    "\n",
    "If we treat $\\theta$ as a random variable then we can say that $\\theta$ is drawn from a prior distribution $P(\\theta)$\n",
    "\n",
    "$P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$\n",
    "\n",
    "\n",
    "and if we assume that pripr is a beta distribution and solve for $theta$ that maximizes $P(\\theta|D)$ we get\n",
    "\n",
    "$\\theta = (n_h +a-1) / (n_h + n_t + a+b-2)$ \n",
    "\n",
    "Basically we are incorporarting our prior knowledge about the data into the model just like we did with the smoothing in the MLE. Doing this is called MAP estimation. Both of them achieve the same result but one think $theta$ is a random variable and the other one thinks it is a parameter.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
